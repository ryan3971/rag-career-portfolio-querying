{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "import openai\n",
    "\n",
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "PROJECTS_DATABASE_ID = os.getenv(\"NOTION_PROJECTS_DATABASE_ID\")\n",
    "EXPERIENCE_DATABASE_ID = os.getenv(\"NOTION_EXPERIENCE_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.getenv(\"CO_API_KEY\") or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM, Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "COLLECTION_TEXT = \"Notion_vector_store_text\"\n",
    "COLLECTION_KEYWORD = \"Notion_vector_store_keywords\"\n",
    "\n",
    "# Set up the LLM\n",
    "llm_openai = OpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.3,\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "Settings.llm = llm_openai\n",
    "\n",
    "# Set up the Embeddings\n",
    "embed_model_openai = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    #api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "Settings.embed_model = embed_model_openai\n",
    "\n",
    "# set up the vector store client\n",
    "client = QdrantClient(\n",
    "    location=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the async client\n",
    "aclient = AsyncQdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the vector store\n",
    "vector_store_text = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_TEXT,\n",
    "    )\n",
    "\n",
    "vector_store_keywords = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_KEYWORD,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up the Notion Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    \"\"\"\n",
    "    A class to process Notion databases and pages, extracting structured content and metadata.\n",
    "    Handles nested databases, headers, lists, and various text block types.\n",
    "    \"\"\"\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database with pagination support.\n",
    "        \n",
    "        Features:\n",
    "        - Handles pagination automatically using Notion's cursor-based system\n",
    "        - Optionally merges parent properties with each page's properties\n",
    "        - Processes all pages in database before returning\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): Notion database ID to query\n",
    "            parent_properties (Dict, optional): Properties to inherit from parent\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All pages in database with merged properties\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent database properties into individual page properties.\n",
    "        \n",
    "        Special handling for different property types:\n",
    "        - Name: Combines parent and child names with separator\n",
    "        - Description: Preserves child description over parent\n",
    "        - Tags: Merges parent and child tags, removing duplicates\n",
    "        - Other: Inherits parent property if not present in child\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Page object to update\n",
    "            parent_properties (Dict): Properties from parent database\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract and normalize page properties from Notion's API response.\n",
    "        \n",
    "        Handles various Notion property types:\n",
    "        - title: Page titles\n",
    "        - rich_text: Multi-line text\n",
    "        - select: Single select options\n",
    "        - multi_select: Multiple select options (converted to comma-separated string)\n",
    "        - date: Date fields (extracts start date)\n",
    "        - number/checkbox: Basic data types\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Raw Notion page object\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Normalized properties with consistent data types\n",
    "        \"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content for consistent formatting.\n",
    "        \n",
    "        Performs the following operations:\n",
    "        1. Replaces multiple spaces with single space\n",
    "        2. Removes spaces before colons\n",
    "        3. Strips whitespace from start/end of lines\n",
    "        4. Removes empty lines\n",
    "        5. Joins cleaned lines with newlines\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text content to normalize\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"\n",
    "        Extract plain text content from Notion's rich text format.\n",
    "        \n",
    "        Features:\n",
    "        - Combines multiple text segments\n",
    "        - Preserves plain text content\n",
    "        - Normalizes whitespace and formatting\n",
    "        \n",
    "        Args:\n",
    "            rich_text (List): Notion rich text array\n",
    "        \n",
    "        Returns:\n",
    "            str: Normalized plain text content\n",
    "        \"\"\"        \n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"\n",
    "        Recursively retrieve all child blocks of a given block.\n",
    "        \n",
    "        Features:\n",
    "        - Handles nested block structure\n",
    "        - Tracks nesting level for proper content organization\n",
    "        - Supports pagination for large block collections\n",
    "        - Skips recursion for child databases (handled separately)\n",
    "        \n",
    "        Args:\n",
    "            block_id (str): ID of block to get children for\n",
    "            level (int): Current nesting level (default: 0)\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[Dict, int]]: List of (block, nesting_level) pairs\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract structured content organized by headers.\n",
    "        \n",
    "        Content organization:\n",
    "        - Level 1 headers start new sections\n",
    "        - Sub-headers are included in section content\n",
    "        - Bullet points are grouped and merged\n",
    "        - Paragraphs are added to current section\n",
    "        \n",
    "        Args:\n",
    "            blocks: List of (block, level) tuples to process\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, List[str]]: \n",
    "                - Dict mapping headers to content section indices\n",
    "                - List of processed content sections\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"\n",
    "        Merge a group of bullet points into a single coherent text chunk.\n",
    "        \n",
    "        Handles nested bullet points by:\n",
    "        - Keeping main (level 0) bullets as separate lines\n",
    "        - Merging sub-bullets inline with their parent bullet\n",
    "        - Preserving the hierarchical relationship in the final text\n",
    "        \n",
    "        Args:\n",
    "            bullet_group: List of (text, level) tuples representing bullet hierarchy\n",
    "        \n",
    "        Returns:\n",
    "            str: Merged bullet points as normalized text\n",
    "        \"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page_whole(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page as a single document without splitting by headers.\n",
    "        \n",
    "        Features:\n",
    "        - Preserves headers as part of content\n",
    "        - Converts bullets to text with bullet markers\n",
    "        - Maintains paragraph structure\n",
    "        - Recursively processes child databases\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List containing single document with full page content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        content_parts = []\n",
    "        \n",
    "        for block, _ in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            if block_type.startswith('heading_'):\n",
    "                # Add headers as text with line breaks\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"{header_text}\\n\")\n",
    "                \n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"â€¢ {text_content}\")\n",
    "                \n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    content_parts.append(text_content)\n",
    "        \n",
    "        # Combine all content\n",
    "        full_content = self._normalize_text('\\n'.join(content_parts))\n",
    "        \n",
    "        if full_content:\n",
    "            results.append({\n",
    "                'properties': properties,\n",
    "                'content': full_content\n",
    "            })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_whole(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page by splitting content at headers.\n",
    "        \n",
    "        Features:\n",
    "        - Creates separate chunks for each header section\n",
    "        - Preserves header hierarchy in properties\n",
    "        - Handles nested databases recursively\n",
    "        - Merges inherited properties appropriately\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of content chunks with associated properties\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_page_granular(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single page with granular text extraction for optimal chunking.\n",
    "        \n",
    "        Key features:\n",
    "        - Plain text blocks are merged into one chunk per header section\n",
    "        - List items are combined with their nested content into separate chunks\n",
    "        - Headers are preserved as metadata properties\n",
    "        - Handles nested databases recursively\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page object to process\n",
    "            parent_properties (Dict, optional): Properties inherited from parent database\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed chunks with properties and content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        current_header = \"Main\"\n",
    "        current_text_chunk = []\n",
    "        current_list = []\n",
    "        in_list = False\n",
    "        \n",
    "        def save_text_chunk():\n",
    "            \"\"\"Helper to save accumulated text chunk\"\"\"\n",
    "            nonlocal current_text_chunk, results, properties, current_header\n",
    "            if current_text_chunk:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_text_chunk))\n",
    "                })\n",
    "                current_text_chunk = []\n",
    "        \n",
    "        def save_list_chunk():\n",
    "            \"\"\"Helper to save accumulated list chunk\"\"\"\n",
    "            nonlocal current_list, results, properties, current_header\n",
    "            if current_list:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_list))\n",
    "                })\n",
    "                current_list = []\n",
    "        \n",
    "        prev_level = 0\n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                save_text_chunk()\n",
    "                save_list_chunk()\n",
    "                in_list = False\n",
    "                current_header = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "            # Handle list items\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                # If this is a new list (not in a list or level decreased)\n",
    "                if not in_list or level < prev_level:\n",
    "                    save_text_chunk()\n",
    "                    save_list_chunk()\n",
    "                    current_list.append(text_content)\n",
    "                    in_list = True\n",
    "                else:\n",
    "                    # Continue existing list\n",
    "                    current_list.append(text_content)\n",
    "                \n",
    "                prev_level = level\n",
    "                \n",
    "            # Handle paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    if in_list:\n",
    "                        # If we're in a list, append to the current list item\n",
    "                        current_list.append(text_content)\n",
    "                    else:\n",
    "                        # Otherwise, add to text chunk\n",
    "                        current_text_chunk.append(text_content)\n",
    "        \n",
    "        # Save any remaining chunks\n",
    "        save_text_chunk()\n",
    "        save_list_chunk()\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_granular(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str, extraction_mode: str = 'header') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process entire database and return structured data.\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): The ID of the Notion database to process\n",
    "            extraction_mode (str): Controls how content is extracted and chunked:\n",
    "                - 'header': splits pages by headers (default)\n",
    "                - 'whole': processes each page as a single document\n",
    "                - 'granular': extracts text blocks and list items separately\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed content chunks, each containing:\n",
    "                - properties: Dict of page metadata\n",
    "                - content: Extracted and normalized text content\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            if extraction_mode == 'header':\n",
    "                processed_data.extend(self.process_page(page))\n",
    "            elif extraction_mode == 'whole':\n",
    "                processed_data.extend(self.process_page_whole(page))\n",
    "            elif extraction_mode == 'granular':\n",
    "                processed_data.extend(self.process_page_granular(page))\n",
    "            else:\n",
    "                raise ValueError(\"extraction_mode must be one of: 'header', 'whole', 'granular'\")\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from the Notion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "processor_projects = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data_projects = processor_projects.process_database(PROJECTS_DATABASE_ID, extraction_mode='header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "processor_experience = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data_experience = processor_experience.process_database(EXPERIENCE_DATABASE_ID, extraction_mode='whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excluded_metadata_keys_projects = [\n",
    "                            #'Employer', \n",
    "                            #'Description', \n",
    "                            'Project Size',\n",
    "                            'When',\n",
    "                            'Position',\n",
    "                            'Tags', \n",
    "                            #'Name',\n",
    "                            #'header'\n",
    "                            ]\n",
    "\n",
    "documents_projects = [Document(text=record['content'], \n",
    "                      metadata=record['properties'], \n",
    "                      excluded_embed_metadata_keys=excluded_metadata_keys_projects, \n",
    "                      excluded_llm_metadata_keys=excluded_metadata_keys_projects) \n",
    "            for record in processed_data_projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excluded_metadata_keys_experience = [\n",
    "                            #'Employer',\n",
    "                            'When',\n",
    "                            #'Skills',\n",
    "                            #'Name'\n",
    "                            ]    \n",
    "\n",
    "documents_experience = [Document(text=record['content'], \n",
    "                      metadata=record['properties'], \n",
    "                      excluded_embed_metadata_keys=excluded_metadata_keys_experience, \n",
    "                      excluded_llm_metadata_keys=excluded_metadata_keys_experience) \n",
    "            for record in processed_data_experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the documents\n",
    "documents = documents_projects + documents_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Keywords to embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.schema import Node, NodeRelationship, RelatedNodeInfo\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from prompts import KEYWORD_PROMPT\n",
    "\n",
    "keyword_prompt = PromptTemplate(KEYWORD_PROMPT)\n",
    "\n",
    "keyword_extractor = KeywordExtractor(\n",
    "    keywords=10, \n",
    "    llm=llm_openai,\n",
    "    prompt=keyword_prompt,\n",
    "    is_text_node_only=True,\n",
    "    metadata_mode=MetadataMode.NONE\n",
    "    )\n",
    "\n",
    "def extract_keywords(node: Node) -> str:\n",
    "    extracted_keywords = keyword_extractor.extract(nodes=[node])[0]\n",
    "    keywords = extracted_keywords['excerpt_keywords']\n",
    "    return keywords\n",
    "\n",
    "def process_documents(documents: List[Document]) -> Tuple[List[Node], List[Node]]:\n",
    "    \"\"\"Process documents into text nodes and keyword nodes.\"\"\"\n",
    "    # Parse documents into nodes\n",
    "    sentence_splitter = SentenceSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=32,\n",
    "        paragraph_separator=\"\\n\\n\\n\",\n",
    "    )\n",
    "    \n",
    "    text_nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    # Create corresponding keyword nodes\n",
    "    keyword_nodes = []\n",
    "    \n",
    "    for text_node in tqdm(text_nodes, desc=\"Generating keywords\"):\n",
    "        \n",
    "        # Extract keywords from the text node\n",
    "        keywords = extract_keywords(text_node)\n",
    "        \n",
    "        # Create a new node for keywords\n",
    "        keyword_node = Node(\n",
    "            text=keywords,\n",
    "            node_id=f\"kw_{text_node.node_id}\",\n",
    "            relationships={\n",
    "                NodeRelationship.PARENT: RelatedNodeInfo(node_id=text_node.node_id)\n",
    "            }\n",
    "        )\n",
    "        keyword_nodes.append(keyword_node)\n",
    "    \n",
    "    return text_nodes, keyword_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents into text nodes and keyword nodes\n",
    "text_nodes, keyword_nodes = process_documents(documents)\n",
    "\n",
    "# Print number of nodes created for verification\n",
    "print(f\"Created {len(text_nodes)} text nodes and {len(keyword_nodes)} keyword nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# assign qdrant vector store to storage context\n",
    "storage_context_keywords = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_keywords,\n",
    "    )\n",
    "\n",
    "storage_context_text = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_text,\n",
    "    )\n",
    "\n",
    "keyword_index = VectorStoreIndex(keyword_nodes, \n",
    "                                 show_progress=True,\n",
    "                                 embed_model=embed_model_openai,\n",
    "                                 storage_context=storage_context_keywords\n",
    "                                 )\n",
    "\n",
    "text_index = VectorStoreIndex(text_nodes, \n",
    "                              show_progress=True,\n",
    "                              embed_model=embed_model_openai,\n",
    "                              storage_context=storage_context_text\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    " # run the retriever on the keyword index\n",
    "retriever_keywords = keyword_index.as_retriever(\n",
    "similarity_threshold=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nodes = retriever_keywords.retrieve(\"Some challenges encountered?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nodes[0].node.relationships[NodeRelationship.PARENT].node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KeywordTableIndex\n",
    "\n",
    "index_keyword_regex = KeywordTableIndex(\n",
    "    nodes=text_nodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index_keyword_regex.retrieve(input=\"List some challenges that were faced?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we need to:\n",
    "- Transform the data (Split large documents into chunks and produce Keywords that are also embedded)\n",
    "- Store in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from prompts import KEYWORD_PROMPT\n",
    "\n",
    "# Define the sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "# define the transform\n",
    "transforms = [sentence_splitter, embed_model_openai]\n",
    "\n",
    "# Ingest into Database\n",
    "IngestionPipeline(\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
