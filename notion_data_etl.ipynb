{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup and configuration\n",
    "# ------------------------------\n",
    "# This notebook performs ETL operations on Notion data and sets up vector storage\n",
    "# Required imports for core functionality\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "import openai\n",
    "\n",
    "# Enable async code execution in notebook environment\n",
    "# Required for llama-parse which uses async/await patterns\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load and validate environment variables\n",
    "# -------------------------------------\n",
    "# Notion API credentials and database IDs\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "PROJECTS_DATABASE_ID = os.getenv(\"NOTION_PROJECTS_DATABASE_ID\") \n",
    "EXPERIENCE_DATABASE_ID = os.getenv(\"NOTION_EXPERIENCE_DATABASE_ID\")\n",
    "\n",
    "# Vector store (Qdrant) connection details\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# API keys for ML services\n",
    "# Fallback to interactive prompt if env vars not set\n",
    "CO_API_KEY = os.getenv(\"CO_API_KEY\") or getpass(\"Enter your Cohere API key: \")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM, Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n",
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for LLM, embeddings and vector store functionality\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Get collection names from environment variables\n",
    "COLLECTION_TEXT = os.getenv(\"COLLECTION_TEXT\")\n",
    "COLLECTION_KEYWORD = os.getenv(\"COLLECTION_KEYWORD\")\n",
    "\n",
    "# Initialize and configure OpenAI LLM\n",
    "# Using gpt-4o-mini model with low temperature for more focused responses\n",
    "llm_openai = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,  # Lower temperature for more deterministic outputs\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# Set LLM as global default in Settings\n",
    "Settings.llm = llm_openai\n",
    "\n",
    "# Initialize and configure OpenAI embeddings model\n",
    "# Using text-embedding-3-large for high quality embeddings\n",
    "embed_model_openai = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# Set embeddings model as global default in Settings\n",
    "Settings.embed_model = embed_model_openai\n",
    "\n",
    "# Initialize Qdrant clients for vector storage\n",
    "# Regular client for synchronous operations\n",
    "client = QdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "# Async client for non-blocking operations\n",
    "aclient = AsyncQdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "# Initialize vector stores for text and keyword collections\n",
    "# Using same client instances for efficiency\n",
    "vector_store_text = QdrantVectorStore(\n",
    "    client=client,\n",
    "    aclient=aclient,\n",
    "    collection_name=COLLECTION_TEXT,\n",
    ")\n",
    "\n",
    "vector_store_keywords = QdrantVectorStore(\n",
    "    client=client,\n",
    "    aclient=aclient,\n",
    "    collection_name=COLLECTION_KEYWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up the Notion Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    \"\"\"\n",
    "    A class to process Notion databases and pages, extracting structured content and metadata.\n",
    "    Handles nested databases, headers, lists, and various text block types.\n",
    "    \"\"\"\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database with pagination support.\n",
    "        \n",
    "        Features:\n",
    "        - Handles pagination automatically using Notion's cursor-based system\n",
    "        - Optionally merges parent properties with each page's properties\n",
    "        - Processes all pages in database before returning\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): Notion database ID to query\n",
    "            parent_properties (Dict, optional): Properties to inherit from parent\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All pages in database with merged properties\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent database properties into individual page properties.\n",
    "        \n",
    "        Special handling for different property types:\n",
    "        - Name: Combines parent and child names with separator\n",
    "        - Description: Preserves child description over parent\n",
    "        - Tags: Merges parent and child tags, removing duplicates\n",
    "        - Other: Inherits parent property if not present in child\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Page object to update\n",
    "            parent_properties (Dict): Properties from parent database\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract and normalize page properties from Notion's API response.\n",
    "        \n",
    "        Handles various Notion property types:\n",
    "        - title: Page titles\n",
    "        - rich_text: Multi-line text\n",
    "        - select: Single select options\n",
    "        - multi_select: Multiple select options (converted to comma-separated string)\n",
    "        - date: Date fields (extracts start date)\n",
    "        - number/checkbox: Basic data types\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Raw Notion page object\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Normalized properties with consistent data types\n",
    "        \"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content for consistent formatting.\n",
    "        \n",
    "        Performs the following operations:\n",
    "        1. Replaces multiple spaces with single space\n",
    "        2. Removes spaces before colons\n",
    "        3. Strips whitespace from start/end of lines\n",
    "        4. Removes empty lines\n",
    "        5. Joins cleaned lines with newlines\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text content to normalize\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"\n",
    "        Extract plain text content from Notion's rich text format.\n",
    "        \n",
    "        Features:\n",
    "        - Combines multiple text segments\n",
    "        - Preserves plain text content\n",
    "        - Normalizes whitespace and formatting\n",
    "        \n",
    "        Args:\n",
    "            rich_text (List): Notion rich text array\n",
    "        \n",
    "        Returns:\n",
    "            str: Normalized plain text content\n",
    "        \"\"\"        \n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"\n",
    "        Recursively retrieve all child blocks of a given block.\n",
    "        \n",
    "        Features:\n",
    "        - Handles nested block structure\n",
    "        - Tracks nesting level for proper content organization\n",
    "        - Supports pagination for large block collections\n",
    "        - Skips recursion for child databases (handled separately)\n",
    "        \n",
    "        Args:\n",
    "            block_id (str): ID of block to get children for\n",
    "            level (int): Current nesting level (default: 0)\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[Dict, int]]: List of (block, nesting_level) pairs\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract structured content organized by headers.\n",
    "        \n",
    "        Content organization:\n",
    "        - Level 1 headers start new sections\n",
    "        - Sub-headers are included in section content\n",
    "        - Bullet points are grouped and merged\n",
    "        - Paragraphs are added to current section\n",
    "        \n",
    "        Args:\n",
    "            blocks: List of (block, level) tuples to process\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, List[str]]: \n",
    "                - Dict mapping headers to content section indices\n",
    "                - List of processed content sections\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"\n",
    "        Merge a group of bullet points into a single coherent text chunk.\n",
    "        \n",
    "        Handles nested bullet points by:\n",
    "        - Keeping main (level 0) bullets as separate lines\n",
    "        - Merging sub-bullets inline with their parent bullet\n",
    "        - Preserving the hierarchical relationship in the final text\n",
    "        \n",
    "        Args:\n",
    "            bullet_group: List of (text, level) tuples representing bullet hierarchy\n",
    "        \n",
    "        Returns:\n",
    "            str: Merged bullet points as normalized text\n",
    "        \"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page_whole(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page as a single document without splitting by headers.\n",
    "        \n",
    "        Features:\n",
    "        - Preserves headers as part of content\n",
    "        - Converts bullets to text with bullet markers\n",
    "        - Maintains paragraph structure\n",
    "        - Recursively processes child databases\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List containing single document with full page content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        content_parts = []\n",
    "        \n",
    "        for block, _ in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            if block_type.startswith('heading_'):\n",
    "                # Add headers as text with line breaks\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"{header_text}\\n\")\n",
    "                \n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"• {text_content}\")\n",
    "                \n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    content_parts.append(text_content)\n",
    "        \n",
    "        # Combine all content\n",
    "        full_content = self._normalize_text('\\n'.join(content_parts))\n",
    "        \n",
    "        if full_content:\n",
    "            results.append({\n",
    "                'properties': properties,\n",
    "                'content': full_content\n",
    "            })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_whole(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page by splitting content at headers.\n",
    "        \n",
    "        Features:\n",
    "        - Creates separate chunks for each header section\n",
    "        - Preserves header hierarchy in properties\n",
    "        - Handles nested databases recursively\n",
    "        - Merges inherited properties appropriately\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of content chunks with associated properties\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_page_granular(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single page with granular text extraction for optimal chunking.\n",
    "        \n",
    "        Key features:\n",
    "        - Plain text blocks are merged into one chunk per header section\n",
    "        - List items are combined with their nested content into separate chunks\n",
    "        - Headers are preserved as metadata properties\n",
    "        - Handles nested databases recursively\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page object to process\n",
    "            parent_properties (Dict, optional): Properties inherited from parent database\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed chunks with properties and content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        current_header = \"Main\"\n",
    "        current_text_chunk = []\n",
    "        current_list = []\n",
    "        in_list = False\n",
    "        \n",
    "        def save_text_chunk():\n",
    "            \"\"\"Helper to save accumulated text chunk\"\"\"\n",
    "            nonlocal current_text_chunk, results, properties, current_header\n",
    "            if current_text_chunk:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_text_chunk))\n",
    "                })\n",
    "                current_text_chunk = []\n",
    "        \n",
    "        def save_list_chunk():\n",
    "            \"\"\"Helper to save accumulated list chunk\"\"\"\n",
    "            nonlocal current_list, results, properties, current_header\n",
    "            if current_list:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_list))\n",
    "                })\n",
    "                current_list = []\n",
    "        \n",
    "        prev_level = 0\n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                save_text_chunk()\n",
    "                save_list_chunk()\n",
    "                in_list = False\n",
    "                current_header = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "            # Handle list items\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                # If this is a new list (not in a list or level decreased)\n",
    "                if not in_list or level < prev_level:\n",
    "                    save_text_chunk()\n",
    "                    save_list_chunk()\n",
    "                    current_list.append(text_content)\n",
    "                    in_list = True\n",
    "                else:\n",
    "                    # Continue existing list\n",
    "                    current_list.append(text_content)\n",
    "                \n",
    "                prev_level = level\n",
    "                \n",
    "            # Handle paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    if in_list:\n",
    "                        # If we're in a list, append to the current list item\n",
    "                        current_list.append(text_content)\n",
    "                    else:\n",
    "                        # Otherwise, add to text chunk\n",
    "                        current_text_chunk.append(text_content)\n",
    "        \n",
    "        # Save any remaining chunks\n",
    "        save_text_chunk()\n",
    "        save_list_chunk()\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_granular(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str, extraction_mode: str = 'header') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process entire database and return structured data.\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): The ID of the Notion database to process\n",
    "            extraction_mode (str): Controls how content is extracted and chunked:\n",
    "                - 'header': splits pages by headers (default)\n",
    "                - 'whole': processes each page as a single document\n",
    "                - 'granular': extracts text blocks and list items separately\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed content chunks, each containing:\n",
    "                - properties: Dict of page metadata\n",
    "                - content: Extracted and normalized text content\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            if extraction_mode == 'header':\n",
    "                processed_data.extend(self.process_page(page))\n",
    "            elif extraction_mode == 'whole':\n",
    "                processed_data.extend(self.process_page_whole(page))\n",
    "            elif extraction_mode == 'granular':\n",
    "                processed_data.extend(self.process_page_granular(page))\n",
    "            else:\n",
    "                raise ValueError(\"extraction_mode must be one of: 'header', 'whole', 'granular'\")\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from the Notion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NotionProcessor for projects database\n",
    "# Using 'header' extraction mode to split pages by headers for more granular content chunks\n",
    "processor_projects = NotionProcessor(NOTION_TOKEN)\n",
    "\n",
    "# Extract and process all project data from Notion database\n",
    "# PROJECTS_DATABASE_ID is defined in environment variables\n",
    "# Returns list of dicts containing page properties and processed content\n",
    "processed_data_projects = processor_projects.process_database(\n",
    "    database_id=PROJECTS_DATABASE_ID,\n",
    "    extraction_mode='header'  # Split by headers for better content organization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NotionProcessor for work experience database\n",
    "# Using 'whole' extraction mode since experience entries are typically shorter\n",
    "# and don't need to be split into chunks\n",
    "processor_experience = NotionProcessor(NOTION_TOKEN)\n",
    "\n",
    "# Extract and process all work experience data from Notion database\n",
    "# EXPERIENCE_DATABASE_ID is defined in environment variables\n",
    "# Returns list of dicts containing page properties and processed content\n",
    "processed_data_experience = processor_experience.process_database(\n",
    "    database_id=EXPERIENCE_DATABASE_ID,\n",
    "    extraction_mode='whole'  # Process each experience as a single document\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data into LlamaIndex Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data_projects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m      6\u001b[0m excluded_metadata_keys_projects \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject Size\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Numerical project size not relevant for semantic search\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen\u001b[39m\u001b[38;5;124m'\u001b[39m,         \u001b[38;5;66;03m# Temporal information not needed for content matching\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Keeping 'Employer', 'Description', 'Name', 'header' as they provide important context\u001b[39;00m\n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create Document objects for each project record using list comprehension\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# - Applies consistent metadata exclusions for both embedding and LLM\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# - Preserves core content and relevant properties\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# - More efficient than iterative approach\u001b[39;00m\n\u001b[0;32m     18\u001b[0m documents_projects \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     Document(\n\u001b[0;32m     20\u001b[0m         text\u001b[38;5;241m=\u001b[39mrecord[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mrecord[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m         excluded_embed_metadata_keys\u001b[38;5;241m=\u001b[39mexcluded_metadata_keys_projects,\n\u001b[0;32m     23\u001b[0m         excluded_llm_metadata_keys\u001b[38;5;241m=\u001b[39mexcluded_metadata_keys_projects\n\u001b[1;32m---> 24\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprocessed_data_projects\u001b[49m\n\u001b[0;32m     25\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_data_projects' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# Define metadata keys to exclude from project documents\n",
    "# These keys are excluded from both embedding and LLM contexts to reduce noise\n",
    "# and focus on the most relevant information\n",
    "excluded_metadata_keys_projects = [\n",
    "    'Project Size',  # Numerical project size not relevant for semantic search\n",
    "    'When',         # Temporal information not needed for content matching\n",
    "    'Position',     # Role information handled elsewhere\n",
    "    'Tags'          # Tags handled separately in keyword extraction\n",
    "    # Keeping 'Employer', 'Description', 'Name', 'header' as they provide important context\n",
    "]\n",
    "\n",
    "# Create Document objects for each project record using list comprehension\n",
    "# - Applies consistent metadata exclusions for both embedding and LLM\n",
    "# - Preserves core content and relevant properties\n",
    "# - More efficient than iterative approach\n",
    "documents_projects = [\n",
    "    Document(\n",
    "        text=record['content'],\n",
    "        metadata=record['properties'],\n",
    "        excluded_embed_metadata_keys=excluded_metadata_keys_projects,\n",
    "        excluded_llm_metadata_keys=excluded_metadata_keys_projects\n",
    "    ) for record in processed_data_projects\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata keys to exclude from experience documents\n",
    "# Only excluding 'When' since temporal information isn't needed for matching\n",
    "# Keeping employer, skills, and name as they provide important context for responses\n",
    "excluded_metadata_keys_experience = [\n",
    "    'When',  # Temporal information not needed for content matching\n",
    "    # keeping employer, skills, and name as they provide important context for responses\n",
    "]    \n",
    "\n",
    "# Create Document objects for each experience record using list comprehension\n",
    "# - Applies consistent metadata exclusions for both embedding and LLM contexts\n",
    "# - Preserves core content and relevant properties like employer, skills, name\n",
    "# - More efficient than iterative approach\n",
    "documents_experience = [\n",
    "    Document(\n",
    "        text=record['content'],\n",
    "        metadata=record['properties'],\n",
    "        excluded_embed_metadata_keys=excluded_metadata_keys_experience,\n",
    "        excluded_llm_metadata_keys=excluded_metadata_keys_experience\n",
    "    ) for record in processed_data_experience\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the documents\n",
    "documents = documents_projects + documents_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Verify the content and metadata that the LLM and Embedding model see\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetadataMode\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe LLM sees this: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mdocuments\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM),\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Embedding model sees this: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     documents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED),\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify the content and metadata that the LLM and Embedding model see\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Keywords to embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required llama-index components\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.schema import Node, NodeRelationship, RelatedNodeInfo, MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from prompts import KEYWORD_PROMPT\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create prompt template for keyword extraction using predefined prompt\n",
    "keyword_prompt = PromptTemplate(KEYWORD_PROMPT)\n",
    "\n",
    "# Initialize keyword extractor with optimized settings\n",
    "keyword_extractor = KeywordExtractor(\n",
    "    keywords=10,  # Number of keywords to extract per node\n",
    "    llm=llm_openai,  # Use OpenAI LLM for extraction\n",
    "    prompt=keyword_prompt,\n",
    "    is_text_node_only=True,  # Only extract from text content for efficiency\n",
    "    metadata_mode=MetadataMode.NONE  # Ignore metadata to reduce processing overhead\n",
    ")\n",
    "\n",
    "def extract_keywords(node: Node) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract keywords from a single node using the configured keyword extractor.\n",
    "    \n",
    "    Args:\n",
    "        node: Input node containing text to extract keywords from\n",
    "        \n",
    "    Returns:\n",
    "        List of extracted keywords as strings\n",
    "    \n",
    "    Note:\n",
    "        Uses batch size of 1 for better control over rate limiting\n",
    "    \"\"\"\n",
    "    extracted_keywords = keyword_extractor.extract(nodes=[node])[0]\n",
    "    return extracted_keywords['excerpt_keywords']\n",
    "\n",
    "def process_documents(documents: List[Document]) -> Tuple[List[Node], List[Node]]:\n",
    "    \"\"\"\n",
    "    Process documents into text nodes and corresponding keyword nodes.\n",
    "    Optimizes chunking and keyword extraction for large document sets.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of input documents to process\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - List of text nodes split from documents \n",
    "        - List of keyword nodes with extracted keywords and parent relationships\n",
    "        \n",
    "    Note:\n",
    "        Uses sentence splitting with overlap to maintain context\n",
    "        Processes in batches with progress tracking\n",
    "    \"\"\"\n",
    "    # Initialize sentence splitter with optimized parameters\n",
    "    sentence_splitter = SentenceSplitter(\n",
    "        chunk_size=512,  # Optimal chunk size for most embedding models\n",
    "        chunk_overlap=32,  # Minimal overlap to maintain context\n",
    "        paragraph_separator=\"\\n\\n\\n\"  # Clear separation between content blocks\n",
    "    )\n",
    "    \n",
    "    # Split all documents into text nodes at once\n",
    "    text_nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    # Pre-allocate keyword nodes list for efficiency\n",
    "    keyword_nodes = []\n",
    "    \n",
    "    # Process nodes with progress tracking\n",
    "    for text_node in tqdm(text_nodes, desc=\"Generating keywords\"):\n",
    "        # Extract keywords and create linked keyword node\n",
    "        keywords = extract_keywords(text_node)\n",
    "        keyword_node = Node(\n",
    "            text=keywords,\n",
    "            node_id=f\"kw_{text_node.node_id}\",\n",
    "            relationships={\n",
    "                NodeRelationship.PARENT: RelatedNodeInfo(node_id=text_node.node_id)\n",
    "            }\n",
    "        )\n",
    "        keyword_nodes.append(keyword_node)\n",
    "    \n",
    "    return text_nodes, keyword_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents into text nodes and keyword nodes\n",
    "# This step splits documents into chunks and extracts keywords for vector search\n",
    "# The process_documents function handles batching and progress tracking internally\n",
    "text_nodes, keyword_nodes = process_documents(documents)\n",
    "\n",
    "# Print node counts for verification and logging\n",
    "# This helps validate that document processing completed successfully\n",
    "# Equal counts indicate 1:1 mapping between text and keyword nodes as expected\n",
    "node_counts = {\n",
    "    'text_nodes': len(text_nodes),\n",
    "    'keyword_nodes': len(keyword_nodes)\n",
    "}\n",
    "print(f\"Created {node_counts['text_nodes']} text nodes and {node_counts['keyword_nodes']} keyword nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required llama-index components\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Create storage contexts for both keyword and text vector stores\n",
    "# This allows us to persist the vector stores and reuse them later\n",
    "storage_context_keywords = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_keywords,\n",
    ")\n",
    "\n",
    "storage_context_text = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_text,\n",
    ")\n",
    "\n",
    "# Create vector index for keyword nodes\n",
    "# This enables semantic search over extracted keywords\n",
    "keyword_index = VectorStoreIndex(\n",
    "    nodes=keyword_nodes,\n",
    "    storage_context=storage_context_keywords,\n",
    "    embed_model=embed_model_openai,\n",
    "    show_progress=True  # Show progress bar for long indexing operations\n",
    ")\n",
    "\n",
    "# Create vector index for text nodes \n",
    "# This enables semantic search over the actual document content\n",
    "text_index = VectorStoreIndex(\n",
    "    nodes=text_nodes,\n",
    "    storage_context=storage_context_text,\n",
    "    embed_model=embed_model_openai,\n",
    "    show_progress=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
